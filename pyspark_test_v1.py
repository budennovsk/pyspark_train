# -*- coding: utf-8 -*-
"""pyspark_test_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fScLIZIVrirAc5ipLy60VFO9rTyDeJDj
"""



from pyspark.sql import SparkSession

# Создание SparkSession
spark = SparkSession.builder \
    .appName("Reading CSV file") \
    .getOrCreate()

# Путь к файлу CSV
file_path = "/content/drive/MyDrive/Colab Notebooks/zvezda/MIO_1/Dominiks_Cheese_DF/wche.csv"

# Чтение файла CSV
df = spark.read.csv(file_path, header=True, inferSchema=True)

# Вывод первых нескольких строк DataFrame
df.show()

# Остановка SparkSession
# spark.stop()

df.printSchema()

# Преобразование Spark DataFrame в Pandas DataFrame
pandas_df = df.limit(10).toPandas()

# Вывод первых 10 строк Pandas DataFrame
print(pandas_df.head(10))

data = [
    {"ddd": 111, "ddf": 22, "ff": 33},
    {"ddd": 44, "ddf": 'dd', "ff": None}
]
# Создание DataFrame с автоматически выведенной схемой
df = spark.createDataFrame(data)
# Вывод схемы DataFrame
df.printSchema()

# Вывод первых нескольких строк DataFrame
df.show()

from datetime import datetime, date
df_1 = spark.createDataFrame([
    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),
    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),
    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))
], schema='a long, b double, c string, d date, e timestamp')
df_1.show()
df_1.printSchema()

spark.conf.set('spark.sql.repl.eagerEval.enabled', True)
df

df.show(1,vertical=True)

df.columns

df.head(2)

df.select(df.columns[:2]).limit(10).collect()

df['STORE']

df.select('STORE').show()

from pyspark.sql.functions import upper,lower
df.withColumn('lower_c', lower(df.SALE)).show()

df.filter(df.WEEK==6).count()

import pandas as pd
from pyspark.sql.functions import pandas_udf

@pandas_udf('long')
def pandas_plus_one(series: pd.Series) -> pd.Series:
    # Simply plus one by using pandas Series.
    return series + 1

df.select(pandas_plus_one(df.STORE)).show()

df.groupby(df.STORE).sum().show()

df_2 = spark.createDataFrame(
    [(14, "Tom"), (23, "Alice"), (16, "Bob")], ["age", "name"])

def func(person):
    return person.age+22


df_2.foreach(func)
df_2

df_2

df.dtypes

# !echo "# pyspark_train" >> README.md
# !git init
# !git add README.md
# !git commit -m "first commit"
# !git branch -M budennovsk/pythonanywere
# !git remote add origin git@github.com:budennovsk/pyspark_train.git
# !git push -u origin budennovsk/pythonanywere
#
